# üêç MapReduce con Python - GU√çA COMPLETA

## ‚ú® MUCHO M√ÅS SIMPLE QUE JAVA

Los mismos 3 an√°lisis pero en Python puro, sin necesidad de compilar.

---

## üìÅ Archivos Creados

### Job 1: Top 10 Productos M√°s Vistos
- `top10_mapper.py` - Filtra eventos "view"
- `top10_reducer.py` - Cuenta y ordena el Top 10

### Job 2: Monto Total por Cliente
- `monto_mapper.py` - Filtra eventos "cart" y extrae precios
- `monto_reducer.py` - Suma montos por cliente

### Job 3: Art√≠culos por Semana
- `semana_mapper.py` - Extrae la semana de cada evento "cart"
- `semana_reducer.py` - Cuenta art√≠culos por semana

---

## üöÄ PASOS R√ÅPIDOS

### 1Ô∏è‚É£ Copiar scripts al contenedor

```bash
docker cp python-mapreduce namenode:/opt/hadoop-3.2.1/
```

### 2Ô∏è‚É£ Conectarse al namenode

```bash
docker exec -it namenode bash
```

### 3Ô∏è‚É£ Ir al directorio

```bash
cd /opt/hadoop-3.2.1/python-mapreduce
chmod +x *.py
```

### 4Ô∏è‚É£ Verificar que los datos est√°n en HDFS

```bash
hadoop fs -ls /tmp/2019-Oct.csv
```

---

## üéØ EJECUTAR LOS JOBS

### Job 1: Top 10 Productos M√°s Vistos

```bash
hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-*.jar \
  -files top10_mapper.py,top10_reducer.py \
  -mapper top10_mapper.py \
  -reducer top10_reducer.py \
  -input /tmp/2019-Oct.csv \
  -output /output/python_top10_productos
```

**Ver resultados:**
```bash
hadoop fs -cat /output/python_top10_productos/part-00000
```

---

### Job 2: Monto Total por Cliente

```bash
hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-*.jar \
  -files monto_mapper.py,monto_reducer.py \
  -mapper monto_mapper.py \
  -reducer monto_reducer.py \
  -input /tmp/2019-Oct.csv \
  -output /output/python_monto_cliente
```

**Ver resultados:**
```bash
hadoop fs -cat /output/python_monto_cliente/part-00000 | head -20
```

---

### Job 3: Art√≠culos en Carrito por Semana

```bash
hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-*.jar \
  -files semana_mapper.py,semana_reducer.py \
  -mapper semana_mapper.py \
  -reducer semana_reducer.py \
  -input /tmp/2019-Oct.csv \
  -output /output/python_articulos_semana
```

**Ver resultados:**
```bash
hadoop fs -cat /output/python_articulos_semana/part-00000
```

---

## üß™ PROBAR LOCALMENTE (Opcional)

Puedes probar los scripts antes de ejecutarlos en Hadoop:

```bash
# Job 1
cat 2019-Oct.csv | ./top10_mapper.py | sort | ./top10_reducer.py

# Job 2
cat 2019-Oct.csv | ./monto_mapper.py | sort | ./monto_reducer.py

# Job 3
cat 2019-Oct.csv | ./semana_mapper.py | sort | ./semana_reducer.py
```

---

## üîß Comandos √ötiles

### Limpiar outputs (si necesitas re-ejecutar):

```bash
hadoop fs -rm -r /output/python_top10_productos
hadoop fs -rm -r /output/python_monto_cliente
hadoop fs -rm -r /output/python_articulos_semana
```

### Descargar resultados:

```bash
# Dentro del contenedor
hadoop fs -get /output/python_top10_productos/part-00000 top10_resultados.txt
hadoop fs -get /output/python_monto_cliente/part-00000 monto_resultados.txt
hadoop fs -get /output/python_articulos_semana/part-00000 semana_resultados.txt

# Desde Windows, copiar del contenedor a tu PC
docker cp namenode:/opt/hadoop-3.2.1/python-mapreduce/top10_resultados.txt ./
docker cp namenode:/opt/hadoop-3.2.1/python-mapreduce/monto_resultados.txt ./
docker cp namenode:/opt/hadoop-3.2.1/python-mapreduce/semana_resultados.txt ./
```

---

## üìä Formato de Resultados

### Job 1: Top 10 Productos
```
1004767    25000
5100500    18500
3902945    15200
...
```
`product_id [TAB] n√∫mero_de_vistas`

### Job 2: Monto por Cliente
```
541312140    350.75
554748717    1250.00
519107250    543.10
...
```
`user_id [TAB] monto_total`

### Job 3: Art√≠culos por Semana
```
2019-W40    5000
2019-W41    7500
2019-W42    8200
2019-W43    6800
```
`a√±o-semana [TAB] n√∫mero_de_art√≠culos`

---

## üí° ¬øC√≥mo funciona Hadoop Streaming?

Hadoop Streaming permite ejecutar scripts de cualquier lenguaje (Python, Ruby, Perl, etc.) como Mappers y Reducers.

**El flujo es:**
1. **Mapper** lee l√≠nea por l√≠nea desde `stdin` y escribe a `stdout`
2. **Hadoop** ordena autom√°ticamente por clave (entre mapper y reducer)
3. **Reducer** recibe datos ordenados desde `stdin` y escribe a `stdout`

**Ventajas:**
- ‚úÖ No necesitas compilar
- ‚úÖ C√≥digo m√°s simple y legible
- ‚úÖ Puedes probar localmente sin Hadoop
- ‚úÖ F√°cil de debuggear

---

## ‚ö†Ô∏è Troubleshooting

### Error: "Permission denied"
```bash
chmod +x *.py
```

### Error: "python3: command not found"
Verifica la versi√≥n de Python en el contenedor:
```bash
python --version
python3 --version
```

Si solo hay `python`, cambia la primera l√≠nea de los scripts:
```python
#!/usr/bin/env python
```

### Error: "Output directory already exists"
```bash
hadoop fs -rm -r /output/python_*
```

### Ver logs de errores:
```bash
# Ver logs del job mientras corre
yarn logs -applicationId <application_id>

# O visita la UI
# http://localhost:8088
```

---

## üéì Explicaci√≥n del C√≥digo

### Estructura de un Mapper:
```python
import sys

for line in sys.stdin:           # Lee l√≠nea por l√≠nea
    # Procesar la l√≠nea
    key, value = procesar(line)
    print(f"{key}\t{value}")     # Emite: clave TAB valor
```

### Estructura de un Reducer:
```python
import sys
from collections import defaultdict

datos = defaultdict(int)

for line in sys.stdin:           # Lee l√≠nea por l√≠nea
    key, value = line.split('\t')
    datos[key] += int(value)     # Agregar/procesar

for key, total in datos.items():
    print(f"{key}\t{total}")     # Emitir resultado
```

---

## üöÄ Script Autom√°tico

Si quieres ejecutar todos los jobs de una vez:

```bash
./run_all.sh
```

Este script ejecuta los 3 jobs secuencialmente y muestra los resultados.

---

## üìà Monitoreo

Mientras corren los jobs, puedes monitorearlos en:

- **Resource Manager UI:** http://localhost:8088
- **NameNode UI:** http://localhost:9870

Aqu√≠ ver√°s:
- Progreso del job (% Map y % Reduce completado)
- Memoria y CPU utilizados
- Logs de errores
- Tiempo de ejecuci√≥n

---

¬°Ya est√° todo listo! üéâ Solo copia los scripts al contenedor y ejecuta los comandos.
